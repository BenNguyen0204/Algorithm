# Have 1 < logn < $/sqrt{n}$ < n < nlogn < n<sup>2</sup> < n<sup>3</sup> < ... < 2<sup>n</sup> < 3<sup>n</sup> < ... < n<sup>n</sup>

# Big-O Notation:

## Definition:
- Describes the **uper bound** on an algorithm's running time by counting how many basic operations it performs as the input size n grows.
- **Formally**: f(n) = O(g(n)) iff there exist some positive constants c and n0 such that f(n) ≤ c*g(n) for all n > n0
    - **Ex**: f(n) = 2n+3
    1) 2n + 3 ≤ 10n for all n ≥ 1
    Here, g(n) = n and c = 10
    => f(n) = O(n)
    2) 2n + 3 ≤ 2n<sup>2</sup> + 3n<sup>2</sup> = 5n<sup>2</sup>
    Here g(n) = O(n<sup>2</sup>) and c = 5, also true
    => f(n) = O(n<sup>2</sup>)
    3) We see that f(n) = O(n) so:
        - **Upper bound** is all the functions: nlogn < n<sup>2</sup> < n<sup>3</sup> < ... < 2<sup>n</sup> < 3<sup>n</sup> < ... < n<sup>n</sup>
        - **Lower bound** is all the functions: 1 < logn < $/sqrt{n}$
        - **Average bound** is n.

# Omega

## Definition:
- Describes the **lower bound** on an algorithm's running time
- **Formally**: f(n) = Ω(g(n)) iff there exist some positive constants c and n0 such that f(n) ≥ c*g(n) for all n > n0

# Theta

## Definition:
- Describes the **tight bound** on an algorithm’s running time
- **Formally**: f(n) = Θ(g(n)) iff there exist some positive constants c1 and c2 and n0 such that c1 * g(n) ≤ f(n) ≤ c2*g(n) for all n > n0
